{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw4Xgf9D41DZ"
      },
      "source": [
        "# Homework 4 (Sequential Models)\n",
        "\n",
        "1. Choose a book or other text to train your model on (I suggest [Project Gutenberg](https://www.gutenberg.org/ebooks/) to find .txt files but you can find them elsewhere). Make sure your file is a `.txt` file. Clean your data (You may use [this file](https://colab.research.google.com/drive/1HCgKn5XQ7Q3ywxGszVWx2kddfT9UBASp?usp=sharing) we talked about in class as a baseline). Build a sequential model (LSTM, GRU, SimpleRNN, or Transformer) that generates new lines based on the training data (NO TRANSFER LEARNING).\n",
        "\n",
        "Print out or write 10 generated sequences from your model (Similar to Classwork 17 where we generated new Pride and Prejudice lines, but now with words instead of charachters. Feel free to use [this](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/) as a reference). Assess in detail how good they are, what they're good at, what they struggle to do well. \n",
        "\n",
        "2. Make a new model with ONE substantial adjustment (e.g. use a custom embedding layer if you didn't already, use a pre-trained embedding layer if you didn't already, use a DEEP LSTM/GRU with multiple recurrent layers, use a pre-trained model to do transfer learning and fine-tune it...etc.). \n",
        "\n",
        "Print out or write 10 generated sequences from your model (Similar to Classwork 17 where we generated new Pride and Prejudice lines, but now with words instead of charachters. Feel free to use [this](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/) as a reference). Assess in detail how good they are, what they're good at, what they struggle to do well.  Did the performance of your model change?\n",
        "\n",
        "3. Then create a **technical report** discussing your model building process, the results, and your reflection on it. The report should follow the format in the example including an Introduction, Analysis, Methods, Results, and Reflection section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AURbTP2S41Dd"
      },
      "source": [
        "# Introduction\n",
        "An introduction should introduce the problem you're working on, give some background and relevant detail for the reader, and explain why it is important. \n",
        "\n",
        "# Analysis \n",
        "Any exploratory analysis of your data, and general summarization of the data (e.g. summary statistics, correlation heatmaps, graphs, information about the data...). This can also include any cleaning and joining you did. \n",
        "\n",
        "# Methods\n",
        "Explain the structure of your model and your approach to building it. This can also include changes you made to your model in the process of building it. Someone should be able to read your methods section and *generally* be able to tell exactly what architechture you used. \n",
        "\n",
        "# Results\n",
        "Detailed discussion of how your model performed, and your discussion of how your model performed.\n",
        "\n",
        "# Reflection\n",
        "Reflections on what you learned/discovered in the process of doing the assignment. Things you would do differently in the future, ways you'll approach similar problems in the future, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANALYSIS:"
      ],
      "metadata": {
        "id": "gobvzJQ87EOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cwd = os.getcwd()\n",
        "print(cwd)\n"
      ],
      "metadata": {
        "id": "G7SIYWJ67iCq",
        "outputId": "bf24f092-ed96-4264-bd3f-f80de57cc32f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import string\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from keras import models\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from sklearn import metrics\n",
        "from tensorflow import keras\n",
        "import pickle\n",
        "from random import randint\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        " # open the file as read only\n",
        " file = open(filename, 'r')\n",
        " # read all text\n",
        " text = file.read()\n",
        " # close the file\n",
        " file.close()\n",
        " return text\n",
        " \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        " # replace '--' with a space ' '\n",
        " doc = doc.replace('--', ' ')\n",
        " # split into tokens by white space\n",
        " tokens = doc.split()\n",
        " # remove punctuation from each token\n",
        " table = str.maketrans('', '', string.punctuation)\n",
        " tokens = [w.translate(table) for w in tokens]\n",
        " # remove remaining tokens that are not alphabetic\n",
        " tokens = [word for word in tokens if word.isalpha()]\n",
        " # make lower case\n",
        " tokens = [word.lower() for word in tokens]\n",
        " return tokens\n",
        " \n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        " data = '\\n'.join(lines)\n",
        " file = open(filename, 'w')\n",
        " file.write(data)\n",
        " file.close()\n",
        " \n",
        "# load document\n",
        "in_filename = 'Romeo_Juliet.txt'\n",
        "doc = load_doc(in_filename)\n",
        "print(doc[:200])\n",
        " \n",
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))\n",
        " \n",
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        " # select sequence of tokens\n",
        " seq = tokens[i-length:i]\n",
        " # convert into a line\n",
        " line = ' '.join(seq)\n",
        " # store\n",
        " sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        " \n",
        "# save sequences to file\n",
        "out_filename ='Romeo_Juliet_seq.txt'\n",
        "save_doc(sequences, out_filename)\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        " # open the file as read only\n",
        " file = open(filename, 'r')\n",
        " # read all text\n",
        " text = file.read()\n",
        " # close the file\n",
        " file.close()\n",
        " return text\n",
        " \n",
        "# load\n",
        "doc = load_doc('Romeo_Juliet_seq.txt')\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]\n",
        "\n",
        "p_train = 0.8\n",
        "\n",
        "n_train = int(X.shape[0]//(1/p_train))\n",
        "X_train = X[0:n_train]\n",
        "y_train = y[0:n_train]\n",
        "X_test = X[n_train:]\n",
        "y_test = y[n_train:]\n",
        "\n"
      ],
      "metadata": {
        "id": "fmtKWpEF6iXM",
        "outputId": "5679eb87-badb-4767-a0c8-60ff15d90210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ï»¿The Project Gutenberg eBook of Romeo and Juliet, by William Shakespeare\n",
            "\n",
            "This eBook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost\n",
            "['project', 'gutenberg', 'ebook', 'of', 'romeo', 'and', 'juliet', 'by', 'william', 'shakespeare', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'romeo', 'and', 'juliet', 'author', 'william', 'shakespeare', 'release', 'date', 'november', 'ebook', 'most', 'recently', 'updated', 'may', 'language', 'english', 'produced', 'by', 'the', 'pg', 'shakespeare', 'team', 'a', 'team', 'of', 'about', 'twenty', 'project', 'gutenberg', 'volunteers', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'romeo', 'and', 'juliet', 'the', 'tragedy', 'of', 'romeo', 'and', 'juliet', 'by', 'william', 'shakespeare', 'contents', 'the', 'prologue', 'act', 'i', 'scene', 'i', 'a', 'public', 'place', 'scene', 'ii', 'a', 'street', 'scene', 'iii', 'room', 'in', 'house', 'scene', 'iv', 'a', 'street', 'scene', 'v', 'a', 'hall', 'in', 'house', 'act', 'ii', 'chorus', 'scene', 'i', 'an', 'open', 'place', 'adjoining', 'garden', 'scene', 'ii', 'garden', 'scene', 'iii', 'friar', 'cell', 'scene', 'iv', 'a', 'street', 'scene', 'v', 'garden', 'scene', 'vi', 'friar', 'cell', 'act', 'iii']\n",
            "Total Tokens: 27986\n",
            "Unique Tokens: 3797\n",
            "Total Sequences: 27935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        " # open the file as read only\n",
        " file = open(filename, 'r')\n",
        " # read all text\n",
        " text = file.read()\n",
        " # close the file\n",
        " file.close()\n",
        " return text\n",
        " \n",
        "# load\n",
        "doc = load_doc('Romeo_Juliet_seq.txt')\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "metadata": {
        "id": "yBhwjsw6XF0y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]\n",
        "\n",
        "p_train = 0.8\n",
        "\n",
        "n_train = int(X.shape[0]//(1/p_train))\n",
        "X_train = X[0:n_train]\n",
        "y_train = y[0:n_train]\n",
        "X_test = X[n_train:]\n",
        "y_test = y[n_train:]\n"
      ],
      "metadata": {
        "id": "gu4WS17lXMgo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model:\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 200, input_length=seq_length))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X, y, batch_size=128, epochs=25)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5JpqR4Z8E_kA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f347cb3-b1ce-4b4d-d3c9-f7f54005db9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 200)           759600    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 512)           1460224   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50, 512)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               787456    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3798)              489942    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,530,118\n",
            "Trainable params: 3,530,118\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 200)           759600    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 512)           1460224   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50, 512)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               787456    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3798)              489942    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,530,118\n",
            "Trainable params: 3,530,118\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "219/219 [==============================] - 44s 155ms/step - loss: 6.7382 - accuracy: 0.0291\n",
            "Epoch 2/25\n",
            "219/219 [==============================] - 14s 64ms/step - loss: 6.4852 - accuracy: 0.0293\n",
            "Epoch 3/25\n",
            "219/219 [==============================] - 11s 51ms/step - loss: 6.4710 - accuracy: 0.0302\n",
            "Epoch 4/25\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 6.4643 - accuracy: 0.0310\n",
            "Epoch 5/25\n",
            "219/219 [==============================] - 9s 40ms/step - loss: 6.4224 - accuracy: 0.0313\n",
            "Epoch 6/25\n",
            "219/219 [==============================] - 9s 40ms/step - loss: 6.3020 - accuracy: 0.0350\n",
            "Epoch 7/25\n",
            "219/219 [==============================] - 10s 44ms/step - loss: 6.1617 - accuracy: 0.0407\n",
            "Epoch 8/25\n",
            "219/219 [==============================] - 10s 44ms/step - loss: 6.0674 - accuracy: 0.0441\n",
            "Epoch 9/25\n",
            "219/219 [==============================] - 9s 42ms/step - loss: 5.9359 - accuracy: 0.0509\n",
            "Epoch 10/25\n",
            "219/219 [==============================] - 9s 42ms/step - loss: 5.7915 - accuracy: 0.0623\n",
            "Epoch 11/25\n",
            "219/219 [==============================] - 8s 38ms/step - loss: 5.6498 - accuracy: 0.0727\n",
            "Epoch 12/25\n",
            "219/219 [==============================] - 9s 40ms/step - loss: 5.5149 - accuracy: 0.0800\n",
            "Epoch 13/25\n",
            "219/219 [==============================] - 10s 44ms/step - loss: 5.3792 - accuracy: 0.0897\n",
            "Epoch 14/25\n",
            "219/219 [==============================] - 9s 39ms/step - loss: 5.2603 - accuracy: 0.0969\n",
            "Epoch 15/25\n",
            "219/219 [==============================] - 8s 38ms/step - loss: 5.1597 - accuracy: 0.1022\n",
            "Epoch 16/25\n",
            "219/219 [==============================] - 8s 38ms/step - loss: 5.0621 - accuracy: 0.1091\n",
            "Epoch 17/25\n",
            "219/219 [==============================] - 9s 40ms/step - loss: 4.9668 - accuracy: 0.1157\n",
            "Epoch 18/25\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 4.8737 - accuracy: 0.1225\n",
            "Epoch 19/25\n",
            "219/219 [==============================] - 9s 39ms/step - loss: 4.8874 - accuracy: 0.1172\n",
            "Epoch 20/25\n",
            "219/219 [==============================] - 8s 39ms/step - loss: 4.7271 - accuracy: 0.1285\n",
            "Epoch 21/25\n",
            "219/219 [==============================] - 8s 39ms/step - loss: 4.6201 - accuracy: 0.1383\n",
            "Epoch 22/25\n",
            "219/219 [==============================] - 9s 41ms/step - loss: 4.5254 - accuracy: 0.1427\n",
            "Epoch 23/25\n",
            "219/219 [==============================] - 8s 38ms/step - loss: 4.4386 - accuracy: 0.1472\n",
            "Epoch 24/25\n",
            "219/219 [==============================] - 8s 38ms/step - loss: 4.3459 - accuracy: 0.1550\n",
            "Epoch 25/25\n",
            "219/219 [==============================] - 8s 39ms/step - loss: 4.2539 - accuracy: 0.1631\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd45f07eb30>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model.h4')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "5nURQkKLio4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d02c8b1-5972-4b71-b28d-b573ed58ba83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJmImAVY7DmD",
        "outputId": "0690de0a-66a1-49be-fc02-4f3204bf6927"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "now all night for lesser cause and been sick lady capulet ay you have been a mousehunt in your time but i will watch you from such watching now exeunt lady capulet and nurse capulet a jealoushood a jealoushood enter servants with spits logs and baskets now fellow there first servant\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.texts_to_sequences([seed_text])[0]"
      ],
      "metadata": {
        "id": "cHoNH1XKNDVf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up the sequence length for later on\n",
        "seq_length = len(lines[0].split()) - 1"
      ],
      "metadata": {
        "id": "uh65KN6jhrEL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists('tokenizer.pkl') and os.path.getsize('tokenizer.pkl') > 0:\n",
        "    # load the tokenizer from file\n",
        "    with open('tokenizer.pkl', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "else:\n",
        "    # create the tokenizer from scratch\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    with open('tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(tokenizer, f)"
      ],
      "metadata": {
        "id": "c4R020Rxj5EY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = []\n",
        "    in_text = seed_text\n",
        "    for _ in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict probabilities for each word\n",
        "        yhat = model.predict(encoded, verbose=0)\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        max_prob_index = np.argmax(yhat)\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == max_prob_index:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)"
      ],
      "metadata": {
        "id": "s-Cf6UmC-lSv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "# generate 10 sequences of length 20\n",
        "for i in range(10):\n",
        "    # generate a random seed text\n",
        "    seed_text = ' '.join(tokenizer.sequences_to_texts([np.random.randint(1, len(tokenizer.word_index), size=seq_length)])[0])\n",
        "    # generate the sequence\n",
        "    sequence = generate_seq(model, tokenizer, seq_length, seed_text,10)\n",
        "    print(f'Seed: {seed_text}\\nSequence: {sequence}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKjONnUIDfe7",
        "outputId": "9608ae7a-7f8f-4d75-eb05-1e4b6708c9e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: p a l m e r s   b r a i n s   s t r e n g t h   c l u b   w o u l d s t   r o y a l t i e s   t e a c h   l o o k   a l l i g a t o r   s h a k e   t r i f l i n g   g y p s y   f i n e   t i d i n g s   d e s p e r a t e   a d d e d   h a s   s a m p s o n   f o u n d a t i o n s   d i d s t   a d j a c e n t   i m p e a c h   v i a l   p r e s t   p o w e r   p e r m a n e n t   i i i   s i n g l e n e s s   c l i m b s   r a t c a t c h e r   r i c h   s o n   b l u b b e r i n g   r e m e d y   w o u n d   s w e e t i n g   s t a g e   s e e m   r o t e   m a n y   m u s t y   r a v e n   u n c l e   m a t r o n   s e r p e n t   t o l d   b e g g a r s   a g a t e s t o n e   l a u r a   h o p e\n",
            "Sequence: bearing the sun will thou not gone to mantua and\n",
            "\n",
            "Seed: j u l i e t   s u m   p l a g u e   v e r o n a   m o d i f i c a t i o n   j a u n c i n g   e n m i t y   l a m e   d e n i e d   l a c e   s w e e t e r   p e n n y   l a i d   s m e l t   g h o s t l y   w i n d o w s   n e x t   n u p t i a l   s w u n g   e m b r a c e   b e l o n g   b r a i n   b e n t   o f f i c i a l   i s   p r o f a n e   a c t i o n   d e r i v a t i v e   s c a r s   r o b e s   i s s u e   h u n d r e d s   r e v e n g e   s e r v a n t   d i s t r a u g h t   b a g g a g e   b r o t h e r   p o s t   i n c l u d e   j o i n t u r e   l i n e a m e n t   n o t i c e   s c a r e   f o u r t e e n   m u s i c i a n   s a v e   w h a t   w o n d e r   v e r y   d a r e\n",
            "Sequence: and i am a man that i will be a\n",
            "\n",
            "Seed: d e l i c i o u s n e s s   d r i z z l e   d r y b e a t   h u r t   d e p a r t e d   a t t e n d s   b o u n d   m i g h t   w i t h   t i m e l e s s   r e s t   o u g h t   i n v o c a t i o n   n o n e   f l o w e d   f o r g i v e   h i g h w a y   d e p a r t   s w a m p   p a l e   a n g u i s h   p r o v i s i o n   b l e s s i n g s   s t r o k e   s t a t e s   t r a c e s   e y e s i g h t   b e a t s   u n f i r m   b l e s s i n g s   b a d   p r e v a i l s   p e r f o r m a n c e s   a f i r e   p r e v e n t   w i s h   c l a s p s   a g i l e   l o n g e r   e x p o r t i n g   w o e   k n o w   e x p e n s e   c r a c k i n g   s t i r s   m a k e s   s a y i n g   p r o j e c t   s p a r i n g   c o u r a g e\n",
            "Sequence: lady capulet what if thou be a man that i\n",
            "\n",
            "Seed: p i n k   t a l l o w f a c e   q u o t e   a c t i v e   n i g h t s   i n c l u d e   g r o u n d   g i v e   s t r e w   f i d d l e s t i c k   b u r n   t a l l   c o l l a r s   v i o l e n t l y   l i k e n e s s   e x c l u s i o n   c h e e r l y   e x p e n s e s   d w e l l   b r i n g   h a s t e   c h i l d r e n   t h e n c e   r o s e s   o u t c r y   u n l u c k y   d i r e c t   f o r s a k e n   g r o w s   f a i n t l y   j o y f u l   s i m p l e   c o u n t s   p r o m i n e n t l y   t a r d y   b o u n d s   s p e a k   w e d n e s d a y   f i n e   d r i n k   s e e i n g   t y b a l t s   i m p e a c h   b r e a t h e   w o u l d   l u c e n t i o   m u s t y   c o p y r i g h t   c h e v e r i l   w i l t\n",
            "Sequence: have the a sea and it that i am sure\n",
            "\n",
            "Seed: a f f o r d   c a l a m i t y   h e l e n a   f a r t h e s t   i n c r e a s i n g   c o o k   e n d a r t   m o u t h   l i g h t n i n g   s w e a r s   s i n g l e n e s s   l o   d o w n l o a d i n g   a c q u a i n t   s o o n e r   t i e s   w e l l s e e m i n g   m i n d   o u g h t   b r a v e   s n a t c h i n g   t o m o r r o w   h e n c e f o r w a r d   r e g i s t e r e d   a n g e l i c a l   p r a c t i c a l l y   h u n t i n g   w a r m t h   s t r a t a g e m s   s e c u r e   r i s e   c o n t a c t   i n t o   m e e t i n g   h a s t   a l l c h e e r i n g   s u c k i n g   w e a l   n i m b l e   b o d i e s   e n v i o u s   u n b o u n d   m u t i n y   p u p i l   b e g i n n i n g   v a n i t y   c o a c h m a k e r s   s t r a t a g e m s   s e n d s   c o c k a h o o p\n",
            "Sequence: what i will be a man that i am satisfied\n",
            "\n",
            "Seed: s o r r o w   s t e a d s   s o l e m n i t y   k n i g h t   d o w n   a d d r e s s   l o t   s t u m b l e s t   g o l d   d a n g e r   d e l a y   s c o p e   a l l i a n c e   r e a r w a r d   a t t e n d s   w o r n   b o u n d l e s s   c o m b i n e   w a g g o n e r   l i g h t s   g a l l o p s   d o v e   b o i s t e r o u s   s w i f t   e s p e c i a l l y   k n o w s   y o u n g   c r o w   p r o m i s e   c i v i l   l a i d   v a l e n t i n e   t e r r o r   y o u t h   h e l p s   m o r r o w   s p y   e a s i l y   f o l l o w i n g   y e   b o i s t e r o u s   t o n g u e s   s t i c k   e x t r e m i t y   a l l s e e i n g   s i m o n   m a n g l e   v e n g e a n c e   s i r   h e r\n",
            "Sequence: and is the capulets paris o here is the watch\n",
            "\n",
            "Seed: c o n s i d e r i n g   l e a n s   t h a n   c o m e s   m a n n e r s   c a r r y   l u s t y   c a l l s   t o o k   t r a n s p a r e n t   e n t r a n c e   o s i e r   e n d u r e   w o u n d   p o m e g r a n a t e   v e s t a l   c o u r a g e   l o l l i n g   b i l l s   w o f u l   h e a d s t r o n g   e x a m i n e   e m p l o y m e n t   s e v e r i t y   d o c t r i n e   m a i d e n h e a d   n e e d y   j u s t l y   d e c k   i n v i t e d   p e r s o n s   f i l e   p r o v i d e d   w o r m s   p r e s t   j e s t s   c o m p l y   o s i e r   f a r e s   m a y s t   t h i g h   m i s t l i k e   b r o k e   f a i l   s p i t   y e s   c o m f o r t a b l e   m o v e d   a d d e d   a p e\n",
            "Sequence: sun should not a man that i will be a\n",
            "\n",
            "Seed: f u r n i s h   s p h e r e s   d i r e c t   c u r e s   o n   s e v e r i t y   h e a r e t h   c r o t c h e t s   s t r i k e   p r o v e s   g r e a t e s t   s t a t e s   t i p s   b e h i n d   c o n t a c t   t h e r e w i t h a l   g r i e f s   w r i t t e n   f i v e   g o o d   d o n o r s   s m o c k   c r u e l   c o u l d   f o l l o w i n g   e m p l o y e e   p r e c i o u s j u i c e d   l e s t   s e a r c h e r s   c o u l d   d e d i c a t e   r e d e e m   p e r f o r m   t h r o u g h   f o r e f i n g e r   e g g   c h o i c e   r e g i s t e r e d   w i n e   c a m e   c o u l d s t   u n c l e   r o p e s   m e t e o r   w e l l   d i s c r e e t   m a i d e n h o o d s   b u t t o n   a w a k i n g   f e s t i v a l\n",
            "Sequence: day and a man that i will be a man\n",
            "\n",
            "Seed: t h i n e   r i d e   r i g h t   c a t s   s t r e a m   d i n e   f r a n c i s   p a c k t h r e a d   m e r i t   p o w e r   c o p y i n g   t a i l   v a i n   l i m i t   w a i t i n g   o n c e   r e p r e s e n t a t i o n s   l e g a l   s o m e t i m e   p r o v i d e   b r a i n   s a u c e   s h o w e r i n g   g l o v e   v i o l e n t l y   g u n   p r i v a t e   f e t t l e   d r u g s   h a r s h   q u o t h   c o v e r t   a l l i a n c e   b e g g a r s   s o f t e s t   p r e v e n t   d e s c e n t   r e s t o r a t i v e   p r o f i t s   s t i l l   c o m p u t e r   c o n t a i n   d a n g e r   p u r p l e   m o c k e r   n o t i n g   k e y s   b u s i n e s s   p e e v i s h   g r o a n s\n",
            "Sequence: doublet and all the sun will not a man that\n",
            "\n",
            "Seed: m e d l a r   t o r c h e s   c o m e   t h o s e   d e w   w o l v i s h r a v e n i n g   l i k e w i s e   s p i t e   w h e r e o n   v e x e d   b e r h y m e   f i n g e r   f o l l o w e r   g y p s y   a m b i g u i t i e s   t o r c h e s   b o y s   d i s c o u r s e s   n o t e   l a m m a s t i d e   f a i r   c o n s u m e   s y c a m o r e   t h r o u g h   v i s a g e   g r i n d s t o n e   b l a z o n   p r a y e r s   d e n o t e   s a i l   g e n e r a t i o n s   s w e e t l y   c r u e l   s e c t i o n   p r e s e n t   c o d e s   e v e r m o r e   b e e t l e b r o w s   e l l   p a r t i n g   s o u n d p o s t   i v   d i s c l a i m e r s   s t a b s   s h r i e k   s w e e t l y   s i m p l e   w i t h d r a w   a b r o a d   p r e v a i l s\n",
            "Sequence: sure i fight and here and be the sun be\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SECOND MODEL:"
      ],
      "metadata": {
        "id": "Vqz-r-GgL9go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "sNYHXcRcR13Q",
        "outputId": "6f00c9ea-e1da-4215-fb92-f3f735972dc2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-82c3b1391a17>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[0mfheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructFileHeader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_FH_SIGNATURE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mstringFileHeader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bad magic number for file header\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzef_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_FH_FILENAME_LENGTH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: Bad magic number for file header"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained embeddings\n",
        "embedding_path = '/content/glove.6B.100d.txt'\n",
        "embedding_dim = 100\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(embedding_path, 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.strip().split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Load the document\n",
        "in_filename = 'Romeo_Juliet_seq.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "# Integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=seq_length,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False))\n",
        "model.add(LSTM(516, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# Compile and fit model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, batch_size=128, epochs=100)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save('model.h5')\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AevLG5Qe6aR_",
        "outputId": "892416f7-81be-4111-9d99-16f668e6b7de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 50, 100)           379800    \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 50, 516)           1273488   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 50, 516)           0         \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 256)               791552    \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3798)              489942    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,967,678\n",
            "Trainable params: 2,587,878\n",
            "Non-trainable params: 379,800\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "219/219 [==============================] - 12s 36ms/step - loss: 6.7097 - accuracy: 0.0284\n",
            "Epoch 2/100\n",
            "219/219 [==============================] - 8s 35ms/step - loss: 6.4811 - accuracy: 0.0322\n",
            "Epoch 3/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 6.4734 - accuracy: 0.0301\n",
            "Epoch 4/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 6.4164 - accuracy: 0.0313\n",
            "Epoch 5/100\n",
            "219/219 [==============================] - 8s 35ms/step - loss: 6.3524 - accuracy: 0.0362\n",
            "Epoch 6/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 6.2585 - accuracy: 0.0422\n",
            "Epoch 7/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 6.1262 - accuracy: 0.0502\n",
            "Epoch 8/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 5.9733 - accuracy: 0.0567\n",
            "Epoch 9/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 5.8303 - accuracy: 0.0620\n",
            "Epoch 10/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 5.7128 - accuracy: 0.0664\n",
            "Epoch 11/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 5.6107 - accuracy: 0.0706\n",
            "Epoch 12/100\n",
            "219/219 [==============================] - 8s 38ms/step - loss: 5.5055 - accuracy: 0.0774\n",
            "Epoch 13/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 5.4052 - accuracy: 0.0806\n",
            "Epoch 14/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 5.3056 - accuracy: 0.0862\n",
            "Epoch 15/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 5.2043 - accuracy: 0.0892\n",
            "Epoch 16/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 5.1012 - accuracy: 0.0938\n",
            "Epoch 17/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 4.9949 - accuracy: 0.0993\n",
            "Epoch 18/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 4.8851 - accuracy: 0.1035\n",
            "Epoch 19/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 4.7844 - accuracy: 0.1106\n",
            "Epoch 20/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 4.6728 - accuracy: 0.1141\n",
            "Epoch 21/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 4.5558 - accuracy: 0.1207\n",
            "Epoch 22/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 4.4393 - accuracy: 0.1268\n",
            "Epoch 23/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 4.3281 - accuracy: 0.1354\n",
            "Epoch 24/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 4.2194 - accuracy: 0.1429\n",
            "Epoch 25/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 4.1085 - accuracy: 0.1531\n",
            "Epoch 26/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 3.9906 - accuracy: 0.1609\n",
            "Epoch 27/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 3.8811 - accuracy: 0.1725\n",
            "Epoch 28/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 3.7748 - accuracy: 0.1851\n",
            "Epoch 29/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 3.6813 - accuracy: 0.1935\n",
            "Epoch 30/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 3.5817 - accuracy: 0.2058\n",
            "Epoch 31/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 3.4754 - accuracy: 0.2240\n",
            "Epoch 32/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 3.3682 - accuracy: 0.2352\n",
            "Epoch 33/100\n",
            "219/219 [==============================] - 8s 38ms/step - loss: 3.2754 - accuracy: 0.2455\n",
            "Epoch 34/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 3.1849 - accuracy: 0.2610\n",
            "Epoch 35/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 3.0953 - accuracy: 0.2747\n",
            "Epoch 36/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 3.0085 - accuracy: 0.2840\n",
            "Epoch 37/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.9278 - accuracy: 0.2996\n",
            "Epoch 38/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.8321 - accuracy: 0.3163\n",
            "Epoch 39/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.7580 - accuracy: 0.3262\n",
            "Epoch 40/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.6775 - accuracy: 0.3425\n",
            "Epoch 41/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.5914 - accuracy: 0.3558\n",
            "Epoch 42/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.5211 - accuracy: 0.3679\n",
            "Epoch 43/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.4500 - accuracy: 0.3854\n",
            "Epoch 44/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.3744 - accuracy: 0.3944\n",
            "Epoch 45/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.3078 - accuracy: 0.4082\n",
            "Epoch 46/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.2343 - accuracy: 0.4214\n",
            "Epoch 47/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.1946 - accuracy: 0.4273\n",
            "Epoch 48/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.0958 - accuracy: 0.4484\n",
            "Epoch 49/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 2.0524 - accuracy: 0.4574\n",
            "Epoch 50/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.9862 - accuracy: 0.4699\n",
            "Epoch 51/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.9361 - accuracy: 0.4832\n",
            "Epoch 52/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.8648 - accuracy: 0.4964\n",
            "Epoch 53/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.8238 - accuracy: 0.5053\n",
            "Epoch 54/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.7575 - accuracy: 0.5228\n",
            "Epoch 55/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.7117 - accuracy: 0.5314\n",
            "Epoch 56/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.6670 - accuracy: 0.5395\n",
            "Epoch 57/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.6134 - accuracy: 0.5523\n",
            "Epoch 58/100\n",
            "219/219 [==============================] - 8s 36ms/step - loss: 1.5789 - accuracy: 0.5593\n",
            "Epoch 59/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.5394 - accuracy: 0.5715\n",
            "Epoch 60/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.4833 - accuracy: 0.5818\n",
            "Epoch 61/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.4407 - accuracy: 0.5925\n",
            "Epoch 62/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.3859 - accuracy: 0.6061\n",
            "Epoch 63/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.3721 - accuracy: 0.6088\n",
            "Epoch 64/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.3165 - accuracy: 0.6202\n",
            "Epoch 65/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.2761 - accuracy: 0.6332\n",
            "Epoch 66/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.2348 - accuracy: 0.6454\n",
            "Epoch 67/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.1971 - accuracy: 0.6527\n",
            "Epoch 68/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.1754 - accuracy: 0.6586\n",
            "Epoch 69/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.1311 - accuracy: 0.6718\n",
            "Epoch 70/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.1112 - accuracy: 0.6749\n",
            "Epoch 71/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.0757 - accuracy: 0.6852\n",
            "Epoch 72/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.0484 - accuracy: 0.6914\n",
            "Epoch 73/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 1.0210 - accuracy: 0.6972\n",
            "Epoch 74/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.9854 - accuracy: 0.7074\n",
            "Epoch 75/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.9608 - accuracy: 0.7129\n",
            "Epoch 76/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.9360 - accuracy: 0.7207\n",
            "Epoch 77/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.9236 - accuracy: 0.7259\n",
            "Epoch 78/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.9113 - accuracy: 0.7266\n",
            "Epoch 79/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.8762 - accuracy: 0.7356\n",
            "Epoch 80/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.8567 - accuracy: 0.7438\n",
            "Epoch 81/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.8167 - accuracy: 0.7532\n",
            "Epoch 82/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.7985 - accuracy: 0.7585\n",
            "Epoch 83/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.7767 - accuracy: 0.7632\n",
            "Epoch 84/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.7627 - accuracy: 0.7701\n",
            "Epoch 85/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.7455 - accuracy: 0.7762\n",
            "Epoch 86/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.7475 - accuracy: 0.7721\n",
            "Epoch 87/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.7313 - accuracy: 0.7767\n",
            "Epoch 88/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.7052 - accuracy: 0.7867\n",
            "Epoch 89/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.6898 - accuracy: 0.7912\n",
            "Epoch 90/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.6800 - accuracy: 0.7906\n",
            "Epoch 91/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.6740 - accuracy: 0.7947\n",
            "Epoch 92/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.6402 - accuracy: 0.8049\n",
            "Epoch 93/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.6316 - accuracy: 0.8053\n",
            "Epoch 94/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.6309 - accuracy: 0.8065\n",
            "Epoch 95/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.6102 - accuracy: 0.8127\n",
            "Epoch 96/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.5876 - accuracy: 0.8191\n",
            "Epoch 97/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.5863 - accuracy: 0.8192\n",
            "Epoch 98/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.5772 - accuracy: 0.8216\n",
            "Epoch 99/100\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.5593 - accuracy: 0.8262\n",
            "Epoch 100/100\n",
            "219/219 [==============================] - 8s 38ms/step - loss: 0.5435 - accuracy: 0.8353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def generate_text(model, tokenizer, seq_length, seed_text, num_words):\n",
        "    result = []\n",
        "    in_text = seed_text\n",
        "    for _ in range(num_words):\n",
        "        # Encode the input sequence\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # Pad the encoded sequence\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # Predict the next word probabilities\n",
        "        yhat_probs = model.predict(encoded, verbose=0)\n",
        "        # Get the index of the predicted word\n",
        "        yhat_index = np.argmax(yhat_probs)\n",
        "        # Map the predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat_index:\n",
        "                out_word = word\n",
        "                break\n",
        "        # Append the predicted word to the result\n",
        "        result.append(out_word)\n",
        "        # Update the input sequence\n",
        "        in_text += ' ' + out_word\n",
        "    return ' '.join(result)\n"
      ],
      "metadata": {
        "id": "n-mIxGSj8eES"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    # Choose a random seed text\n",
        "    seed_text = lines[randint(0,len(lines))]\n",
        "    # Generate a sequence of 10 words\n",
        "    generated = generate_text(model, tokenizer, seq_length, seed_text, 10)\n",
        "    print(f'Seed: {seed_text}\\nSequence: {generated}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Ub_z5f9a4J",
        "outputId": "722ea94e-78ae-4d9e-ce2b-6c795f771640"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: merry up the child quoth he thou fall upon thy face thou wilt fall backward when thou hast more wit wilt thou not and by my holidame the pretty wretch left crying and said to see now how a jest shall come about i warrant and i should live a thousand\n",
            "Sequence: years i never should forget it thou not quoth he\n",
            "\n",
            "Seed: awake as from a pleasant sleep now when the bridegroom in the morning comes to rouse thee from thy bed there art thou dead then as the manner of our country is in thy best robes on the bier thou shalt be borne to that same ancient vault where all the\n",
            "Sequence: kindred of the capulets lie in the meantime against thou\n",
            "\n",
            "Seed: death calling death banished thou my head off with a golden axe and smilest upon the stroke that murders me friar lawrence o deadly sin o rude unthankfulness thy fault our law calls death but the kind prince taking thy part hath aside the law and that black word death to\n",
            "Sequence: banishment this is dear mercy and thou it not romeo\n",
            "\n",
            "Seed: my ghost seeking out romeo that did spit his body upon a point stay tybalt stay romeo romeo romeo drink i drink to thee throws herself on the bed scene iv hall in house enter lady capulet and nurse lady capulet hold take these keys and fetch more spices nurse nurse\n",
            "Sequence: they call for dates and quinces in the pastry enter\n",
            "\n",
            "Seed: me therefore turn and draw romeo i do protest i never thee but love thee better than thou canst devise till thou shalt know the reason of my love and so good capulet which name i tender as dearly as mine own be satisfied mercutio o calm dishonourable vile submission draws\n",
            "Sequence: alla stoccata carries it away tybalt you ratcatcher will you\n",
            "\n",
            "Seed: how he dares being dared mercutio alas poor romeo he is already dead stabbed with a white black eye run through the ear with a love song the very pin of his heart cleft with the blind buttshaft and is he a man to encounter tybalt benvolio why what is tybalt\n",
            "Sequence: mercutio more than prince of cats o the courageous captain\n",
            "\n",
            "Seed: more than tears with that report juliet that is no slander sir which is a truth and what i spake i spake it to my face paris thy face is mine and thou hast it juliet it may be so for it is not mine own are you at leisure holy\n",
            "Sequence: father now or shall i come to you at evening\n",
            "\n",
            "Seed: thy love prove likewise variable romeo what shall i swear by juliet do not swear at all or if thou wilt swear by thy gracious self which is the god of my idolatry and believe thee romeo if my dear juliet well do not swear although i joy in thee i\n",
            "Sequence: have no joy of this contract tonight it is too\n",
            "\n",
            "Seed: defective or damaged disk or other medium a computer virus or computer codes that damage or cannot be read by your equipment limited warranty disclaimer of damages except for the right of replacement or refund described in paragraph the project gutenberg literary archive foundation the owner of the project gutenbergtm trademark\n",
            "Sequence: and any other party distributing a project gutenbergtm electronic work\n",
            "\n",
            "Seed: that is because the traitor murderer lives juliet ay madam from the reach of these my hands would none but i might venge my death lady capulet we will have vengeance for it fear thou not then weep no more send to one in mantua where that same runagate doth live\n",
            "Sequence: shall give him such myself or for thou meanest not\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JLb87JjXEL23",
        "c1qrDMSfcfB8",
        "ruoBF85Ydr4C"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}